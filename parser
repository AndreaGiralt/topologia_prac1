#!/usr/bin/env python3
import argparse
from scrapy.crawler import CrawlerProcess
from spiders.details import DetalleParser
from spiders.search import SearchSpider
from spiders.zones import ZonesSpider
from libs.csv import create_csv

def main():

    parser = argparse.ArgumentParser(prog="parser",description='Scraper de la web pisos.com (Mallorca)')
    parser.add_argument('-a', '--all', help='Importa todo', action="store_true")
    parser.add_argument('-z','--zones', help='Importa las zonas de Mallorca.', action="store_true")
    parser.add_argument('-p','--previews', help='Importa datos básicos de los anuncios', action="store_true")
    parser.add_argument('-d','--details',help='Importa los detalles de los anuncios.', action="store_true")
    parser.add_argument('-c','--csv', help='Crea un csv con los resultados.',action="store_true")

    args = parser.parse_args()

    if not any(vars(args).values()):
        parser.error('Es necesario indicar como mínimo una acción')

    if args.zones or args.previews or args.details:

        process = CrawlerProcess()

        if args.zones or args.all:
            print("Importando zonas...")
            process.crawl(ZonesSpider)

        if args.previews or args.all:
            print("Importando previsualizaciones...")
            process.crawl(SearchSpider)

        if args.details or args.all:
            print("Importando detalles...")
            process.crawl(DetalleParser)

        process.start()

    elif args.csv:
        print("Creando csv...")
        create_csv()

if __name__ == "__main__":
    main()